---
title: 'Assignment 2: Data visualization'
author: "Tamas Nagy"
output: html_document
editor_options: 
  chunk_output_type: console
---

You will have to create 3 plots based on the datasets and instructions detailed below. You will find the plots themeselves in the `assignments/assignment_2_plots`. Your task is to write the code that will reproduce the plots as closely as possible.

# Skills needed to solve this assignment

-   Using R and RStudio, reading data
-   Reporting using RMarkdown
-   Using Git and Github (for submitting the task)
-   Data manipulation (e.g. dplyr, tidyr), and working with factors (forcats)
-   Data visuzlization (ggplot2)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(tidytuesdayR)
library(viridis)
library(scales)
library(RColorBrewer)
```

## Task 1: Climbing expeditions

The 2020-09-22 TidyTueday datasets are about climbing expeditions. From the three datasets, use the "expeditions". Reproduce the plot below! Notice a few things:

-   Use `forcats::fct_lump()` to get the 15 most frequent peaks, and drop the "Other" category.
-   The bars are ordered by the sum of all expeditions (use `fct_reorder()`).
-   The bar colors use the viridis palette and light theme.

```{r}


# Load the expeditions dataset 
expedition_data <- read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/expeditions.csv",
  show_col_types = FALSE
)


# Prepare data
climbing_data <- expedition_data %>%
  mutate(
    peak_label = forcats::fct_lump(peak_name, n = 15),
    peak_label = droplevels(peak_label)
  ) %>%
  filter(peak_label != "Other")

climbing_counts <- climbing_data %>%
  count(peak_label, season) %>%
  mutate(peak_label = forcats::fct_reorder(peak_label, n, .fun = sum))

# Plot
ggplot(climbing_counts, aes(x = n, y = peak_label, fill = season)) +
  geom_col() +
  scale_fill_viridis_d() +
  labs(
    title = "Top 15 Peaks by Expedition Season",
    x = "Number of Expeditions",
    y = "Peak Name",
    fill = "Season"
  ) +
  theme_light() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_text(hjust = 1),
    legend.position = "bottom",
    legend.title = element_text(hjust = 0.5),
    legend.justification = "center"
  )

```

## Task 2: PhDs awarded

The 2019-02-19 TidyTueday dataset is about phd-s awarded by year and field. There is only one dataset, it is called `phd_by_field`. Reproduce the plot below!

Notes:

-   First you have to aggregate the data to count all phd-s by broad fields.
-   To make the x axis breaks pretty, use `scales::pretty_breaks()`, to make the y axis labels comma formatted, use `scales::comma_format()`.
-   The line size is 1.2, the colors are from the brewer "Dark2" palette. The theme is set to minimal.

```{r}

# Load the PhD data
phd_dataset <- read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-19/phd_by_field.csv",
  show_col_types = FALSE
)

# Aggregate data
phd_totals <- phd_dataset %>%
  group_by(category = broad_field, year) %>%
  summarize(total_count = sum(n_phds, na.rm = TRUE), .groups = "drop")

# Plot
ggplot(phd_totals, aes(x = year, y = total_count, color = category, group = category)) +
  geom_line(size = 1.2) +
  scale_x_continuous(breaks = pretty_breaks()) +
  scale_y_continuous(labels = comma_format()) +
  scale_color_brewer(palette = "Dark2") +
  labs(
    title = "Number of awarded Ph.D.-s in the US by year",
    color = "Broad field"
  ) +
  ylab(NULL) +
  theme_minimal()
```

## Task 3: Commute in the US

The 2019-11-05 TidyTueday dataset is about commuting to work in each city in the US by bike or on foot. There is only one dataset, it is called `commute`. Reproduce the plot below!

Notes:

-   First you have to aggregate the data to count all commutes by state.
-   Both axis scales are log transformed and the labels comma formatted, using `scales::comma_format()`
-   The point size is 2, . The theme is set to light.

```{r}
# Load data 
commute_dataset <- read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-11-05/commute.csv",
  show_col_types = FALSE
)

# Aggregate the data to count all commutes by state
commute_summary <- commute_dataset %>%
  filter(mode %in% c("Bike", "Walk")) %>%
  pivot_wider(
    names_from = mode,
    values_from = n,
    values_fill = list(n = 0)
  ) %>%
  group_by(state_code = state_abb, region = state_region) %>%
  summarize(
    avg_walk = mean(Walk, na.rm = TRUE),
    avg_bike = mean(Bike, na.rm = TRUE),
    .groups = "drop"
  )

# Create the plot
ggplot(commute_summary, aes(x = avg_walk, y = avg_bike, color = region, label = state_code)) +
  geom_point(size = 2) +  
  geom_text(color = "black", size = 4) +  
  scale_x_log10(labels = comma_format()) +
  scale_y_log10(labels = comma_format()) +
  labs(
    title = "Title number of people walking vs. biking to work in each USA state",
    x = "Number of ppl walking to work (log N)",
    y = "Number of ppl biking to work (log N)",
    color = "Region"
  ) +
  theme_light() 

```
